# -*- coding: utf-8 -*-
"""Clasificador Imagenes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3EHAGgjbRICb6p_-zpbguZOWhDWgDMw
"""

from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint

import numpy as np
import matplotlib.pyplot as plt

"""Carga del data set"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Cantidad de datos en train 50,000 imagnes de 32 x 32 de 3 modulos a color
x_train.shape

plt.imshow(x_train[45])

"""Limpieza de datos"""

# pasamos a float32
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#Obtenemos el numero de clases que hay en la salida // hay 10 tipos de clases
num_classes = np.unique(y_train).shape[0]
num_classes

# convertimos la salida escalar a un arreglo
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

y_train[0]

"""Normalizacion"""

# Dividimos entre 255 par aque tenga valores de 0 - 1
#x_train/=255
#x_test/=255

mean = np.mean(x_train)
std = np.std(x_train)

x_train = (x_train - mean) / (std+1e-7)
x_test = (x_test - mean) / (std+1e-7)

x_train[0]

"""Creando set de datos de entrenamiento y validacion"""

# x_train endra los valores del 10,000 al 50 ,000 y el x_val del 0 - 9,999
(x_train,x_valid) = x_train[10000:] , x_train[:10000]
(y_train,y_valid) = y_train[10000:] , y_train[:10000]

"""#### Batch Normalization

Aplicación de Batch Normalization:
Con la normalización por lotes, se calcula la media y la desviación estándar de las activaciones en cada mini lote de datos que pasa por la red. Luego, se normalizan las activaciones utilizando la fórmula mencionada anteriormente. Además, se introducen parámetros aprendidos 𝛾 γ (escala) y 𝛽 β (sesgo) para ajustar las activaciones normalizadas según sea necesario.

Efecto de Batch Normalization:
Después de aplicar la normalización por lotes, las distribuciones de activaciones se vuelven más estables y centradas alrededor de cero. Esto facilita el entrenamiento de la red y acelera la convergencia.

La normalización por lotes ayuda a mantener las activaciones de las capas ocultas en rangos más estables y cercanos a cero, lo que mejora la estabilidad y velocidad de entrenamiento de las redes neuronales profundas.

## Contruyendo nuestros modelos convolucionales
"""

def convolucional1(filtros,df,regulatizer):

  model = Sequential()

  #Conv1
  model.add(Conv2D(filtros,(3,3),padding='same',kernel_regularizer=regularizers.l2(regulatizer),input_shape=df.shape[1:]))
  model.add(Activation('relu'))
  model.add(BatchNormalization()) # Normalizacion de los datos


  #conv2
  model.add(Conv2D(filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())



  #conv3
  model.add(Conv2D(2*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.4))
  model.add(BatchNormalization())

  #conv4
  model.add(Conv2D(2*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(Dropout(0.4))
  model.add(BatchNormalization())

  #conv5
  model.add(Conv2D(4*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())

  #conv6
  model.add(Conv2D(4*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())

  #clasificacion y flatten
  model.add(Flatten())
  model.add(Dense(10,activation='softmax'))

  return model

model = convolucional1(32,x_train,8e-5)

"""### Data argumentation

Funciona para generar mas imagenes con caracterizticas similares a la orgiinal para tener mejores resultados
"""

datagen = ImageDataGenerator(rotation_range=15,
                  width_shift_range=0.1,
                  height_shift_range=0.1,
                  horizontal_flip=True,
                  vertical_flip=True)

"""compilando"""

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

"""### Callbacks"""

chekcpoint = ModelCheckpoint('mi_mejor_modelo.keras',verbose=1,save_best_only=True, monitor = 'val_accuracy')

"""Entrenamiento"""

hist = model.fit(datagen.flow(x_train, y_train, batch_size=128),
          callbacks=[chekcpoint],
          steps_per_epoch=x_train.shape[0] // 128, # para obtener el numero de lotes completos y no realice lotes al infinito
          epochs=70,
          verbose=2,
          validation_data=(x_valid, y_valid)
         )

#hist = model.fit(x_train, y_train, batch_size=64,epochs=50,
#               validation_data=(x_valid, y_valid),
 #              verbose=2, shuffle=True)

"""Resultados"""

plt.plot(hist.history['accuracy'],label='Train')
plt.plot(hist.history['val_accuracy'],label='Val')
plt.legend()
plt.show()

"""Evaluacion del odelo"""

model.evaluate(x_test,y_test) # sin callbacks ni data generator ni batch optimazier

from keras.models import clone_model # para copiar el elemento y no solo la direccion de memoria
model2 = clone_model(model)

model2.load_weights('mi_mejor_modelo.keras')

model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model2.evaluate(x_test,y_test)