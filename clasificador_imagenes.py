# -*- coding: utf-8 -*-
"""Clasificador Imagenes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3EHAGgjbRICb6p_-zpbguZOWhDWgDMw
"""

from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint

import numpy as np
import matplotlib.pyplot as plt

"""Carga del data set"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Cantidad de datos en train 50,000 imagnes de 32 x 32 de 3 modulos a color
x_train.shape

plt.imshow(x_train[45])

"""Limpieza de datos"""

# pasamos a float32
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#Obtenemos el numero de clases que hay en la salida // hay 10 tipos de clases
num_classes = np.unique(y_train).shape[0]
num_classes

# convertimos la salida escalar a un arreglo
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

y_train[0]

"""Normalizacion"""

# Dividimos entre 255 par aque tenga valores de 0 - 1
#x_train/=255
#x_test/=255

mean = np.mean(x_train)
std = np.std(x_train)

x_train = (x_train - mean) / (std+1e-7)
x_test = (x_test - mean) / (std+1e-7)

x_train[0]

"""Creando set de datos de entrenamiento y validacion"""

# x_train endra los valores del 10,000 al 50 ,000 y el x_val del 0 - 9,999
(x_train,x_valid) = x_train[10000:] , x_train[:10000]
(y_train,y_valid) = y_train[10000:] , y_train[:10000]

"""#### Batch Normalization

Aplicaci칩n de Batch Normalization:
Con la normalizaci칩n por lotes, se calcula la media y la desviaci칩n est치ndar de las activaciones en cada mini lote de datos que pasa por la red. Luego, se normalizan las activaciones utilizando la f칩rmula mencionada anteriormente. Adem치s, se introducen par치metros aprendidos 洧 풥 (escala) y 洧띻  (sesgo) para ajustar las activaciones normalizadas seg칰n sea necesario.

Efecto de Batch Normalization:
Despu칠s de aplicar la normalizaci칩n por lotes, las distribuciones de activaciones se vuelven m치s estables y centradas alrededor de cero. Esto facilita el entrenamiento de la red y acelera la convergencia.

La normalizaci칩n por lotes ayuda a mantener las activaciones de las capas ocultas en rangos m치s estables y cercanos a cero, lo que mejora la estabilidad y velocidad de entrenamiento de las redes neuronales profundas.

## Contruyendo nuestros modelos convolucionales
"""

def convolucional1(filtros,df,regulatizer):

  model = Sequential()

  #Conv1
  model.add(Conv2D(filtros,(3,3),padding='same',kernel_regularizer=regularizers.l2(regulatizer),input_shape=df.shape[1:]))
  model.add(Activation('relu'))
  model.add(BatchNormalization()) # Normalizacion de los datos


  #conv2
  model.add(Conv2D(filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())



  #conv3
  model.add(Conv2D(2*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.4))
  model.add(BatchNormalization())

  #conv4
  model.add(Conv2D(2*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(Dropout(0.4))
  model.add(BatchNormalization())

  #conv5
  model.add(Conv2D(4*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())

  #conv6
  model.add(Conv2D(4*filtros,(3,3),padding='same'))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())

  #clasificacion y flatten
  model.add(Flatten())
  model.add(Dense(10,activation='softmax'))

  return model

model = convolucional1(32,x_train,8e-5)

"""### Data argumentation

Funciona para generar mas imagenes con caracterizticas similares a la orgiinal para tener mejores resultados
"""

datagen = ImageDataGenerator(rotation_range=15,
                  width_shift_range=0.1,
                  height_shift_range=0.1,
                  horizontal_flip=True,
                  vertical_flip=True)

"""compilando"""

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

"""### Callbacks"""

chekcpoint = ModelCheckpoint('mi_mejor_modelo.keras',verbose=1,save_best_only=True, monitor = 'val_accuracy')

"""Entrenamiento"""

hist = model.fit(datagen.flow(x_train, y_train, batch_size=128),
          callbacks=[chekcpoint],
          steps_per_epoch=x_train.shape[0] // 128, # para obtener el numero de lotes completos y no realice lotes al infinito
          epochs=70,
          verbose=2,
          validation_data=(x_valid, y_valid)
         )

#hist = model.fit(x_train, y_train, batch_size=64,epochs=50,
#               validation_data=(x_valid, y_valid),
 #              verbose=2, shuffle=True)

"""Resultados"""

plt.plot(hist.history['accuracy'],label='Train')
plt.plot(hist.history['val_accuracy'],label='Val')
plt.legend()
plt.show()

"""Evaluacion del odelo"""

model.evaluate(x_test,y_test) # sin callbacks ni data generator ni batch optimazier

from keras.models import clone_model # para copiar el elemento y no solo la direccion de memoria
model2 = clone_model(model)

model2.load_weights('mi_mejor_modelo.keras')

model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model2.evaluate(x_test,y_test)