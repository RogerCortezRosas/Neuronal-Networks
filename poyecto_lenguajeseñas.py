# -*- coding: utf-8 -*-
"""poyecto_lenguajeSeñas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwTm7Y9mK34alJbikaL0u85QzqhVu7ve

# PROYECTO DE CURSO (Main Project)
## Descrición del lenguaje de señas MNIST (American Sign Language)
Este conjunto de datos se adoptó del lenguaje de señas MNIST, convirtiendo el archivo CSV en imágenes y también disminuyendo el tamaño general de la base de datos.

Hay un total de 27,455 imágenes en escala de grises de tamaño 28 * 28 píxeles cuyo valor oscila entre 0-255. Cada caso representa una etiqueta (0-25) como un mapa uno a uno para cada letra alfabética A-Z (y ningún caso para 9 = J o 25 = Z debido a movimientos gestuales).

Los datos se almacenan de forma ordenada y son compatibles para su uso con generadores de flujo de datos en la API de TensorFlow. Cada carpeta recibe un nombre de acuerdo con la clase de imágenes almacenadas en su interior, lo que facilita su carga y visualización.

Las imágenes se almacenan en formato de archivo 'JPEG'.

Los datos originales de la imagen del gesto de la mano representaban a varios usuarios que repitieron el gesto con diferentes fondos. Los datos del MNIST en lenguaje de señas provienen de una gran extensión del pequeño número (1704) de las imágenes en color incluidas como no recortadas alrededor de la región de interés de la mano.

Para crear nuevos datos, se usó una canalización de imágenes basada en ImageMagick e incluyó recortar a solo manos, escalar grises, cambiar el tamaño y luego crear al menos más de 50 variaciones para aumentar la cantidad. La estrategia de modificación y expansión fueron los filtros ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), junto con un 5% de pixelación aleatoria, +/- 15% de brillo / contraste y finalmente 3 grados de rotación. Debido al pequeño tamaño de las imágenes, estas modificaciones alteran efectivamente la resolución y la separación de clases de formas interesantes y controlables.

Fuente: TecPerson - Kaggle

## Obtencion de archivo de imagenes dede Google Cloud Platform
"""

!wget --no-check-certificate https://storage.googleapis.com/platzi-tf2/sign-language-img.zip \
    -O /tmp/sign-language-img.zip

"""## Descomprimir el archivo

En el siguiente código de Python utilizamos la libreria OS para poder dar acceso a los archivos del sistema operativo y luego con la librería ZipFile descomprimimos la base de datos
"""

import os
import zipfile

local_zip = '/tmp/sign-language-img.zip' # ruta que pusiste en el wget
zip_ref = zipfile.ZipFile(local_zip, 'r') # se cra objeto zip_ref para leer el archivo
zip_ref.extractall('/tmp') # extrae los archivos en el directorio /tmp
zip_ref.close() # IMPORTANTE CERRAR LA SESION

"""## Importamos librerias"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import string
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## obtenemos la direcion de nuestras carpetas extraidas"""

train_dir = "/tmp/sign-language-img/Train"
test_dir = "/tmp/sign-language-img/Test"

"""## Data Generators
Configuremos generadores de datos que leerán imágenes en nuestras carpetas de origen, las convertirán en tensores `float32` y las alimentarán (con sus etiquetas) a nuestra red. Tendremos un generador para las imágenes de entrenamiento y otro para las imágenes de validación. Nuestros generadores producirán lotes de imágenes de tamaño 28x28 y sus etiquetas (clases lenguaje de señas).
"""

# Normalizacion de los datos para que los pixeles esten de 0-1
train_datagen = ImageDataGenerator(rescale = 1/255)
test_datagen = ImageDataGenerator(rescale = 1/255, validation_split= 0.2) # validation_split reserva 20% de los datos para validacion