# -*- coding: utf-8 -*-
"""RedNeuronal_Regresion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NgdWzBTOib2v1X9lt2-uhdUEaLTwaUvg

La data se trata de las caracteristicas de las casas par predecir su precio segun estas caracteristicas
"""

import pandas as pd
import numpy as np
from keras.datasets import boston_housing
from keras import models, layers, optimizers

"""# Descargando datos"""

(train_data , train_targets) ,(test_data,test_targets) = boston_housing.load_data()

train_data[1]

train_targets[1]

"""# Normalizacion

Se hace normalizacion ya que los valores de las diferentes caracteristicas varia mucho y no queremos que le de mas importancia a una caracteristica solo porque su valor numerico es mas alto lo cual no es del todo correcto
"""

mean = train_data.mean(axis=0)
train_data = train_data  - mean
std = train_data.std(axis=0)
train_data = train_data / std

test_data = test_data - mean # se utiliza la media de train porque la data de test es algo que el modelo no tiene que conocer para hacer una evaluacion correcta
test_data = test_data / std

"""#Creacion de la red neuronal"""

from keras import regularizers

def build_model_regression(input_data):
    model = models.Sequential()
    model.add(layers.Dense(128,activation='relu',input_shape=(input_data,)))
    model.add(layers.Dense(64,activation='relu'))

    model.add(layers.Dense(32,activation='relu'))

    model.add(layers.Dense(1)) # como la salida es lineal no necesita una funcion de activacion
    model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), loss='mse',metrics=['mae'])#'mae' Mean Absolute Error Error absoluto medio
    return model

"""# Entrenamiento con K - fold validation"""

k = 4 # numero de folds
num_val_samples = len(train_data) // 4 # numero del tamano del conjunto de validacion
num_epoch = 70
all_history = [] # lista que almacena el historia del mse de cada epoca

train_data[:0]

num_val_samples*2

for i in range(k):
    print("Fold " , i)

    #Conjunto de validacion -> 0 -101 , 101 - 202 , 202 - 303 , 303 -404
    val_data = train_data[i*num_val_samples: (i+1) * num_val_samples]
    val_targets = train_targets[i*num_val_samples: (i+1) * num_val_samples]

    #Conjunto de entrenamiento -> 101 - 404 , 0-101 - 202-404 , 0-202 - 303-404 , 0-303
    partial_train_data = np.concatenate(
    [train_data[:i * num_val_samples],
     train_data[(i+1) * num_val_samples:]],
     axis= 0
    )

    partial_train_targets = np.concatenate(
    [train_targets[:i * num_val_samples],
     train_targets[(i+1) * num_val_samples:]],
     axis= 0
    )
    model = build_model_regression(13)
    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epoch, batch_size =16,
                        validation_data = (val_data, val_targets),
                        verbose=0)
    all_history.append(history.history['val_mae'])

len(all_history) # Son los resultados de los 4 folds

len(all_history[0]) # Son el rsultado de las 80 epocas

# visulaizamos el hitory con un dataframe
# Se puede visualizar como va disminuyendo el mse conforme avanzan las epocas en cada fold
df = pd.DataFrame(all_history)
df

# Obtenemos la media  de cada fold
all_mae_avg = df.mean(axis=0)
all_mae_avg

"""# Visualizacion de resultados"""

import matplotlib.pyplot as plt

import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10,10))
plt.plot(range(1,len(all_mae_avg[15:])+1), all_mae_avg[15:])

plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()

"""# Evaluacion del modelo"""

model.evaluate(test_data,test_targets)

"""# Normalizacion y entrenmiento con diferentes tecnicas"""

from scikeras.wrappers import KerasRegressor

from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import make_scorer, mean_squared_error,mean_absolute_error
from sklearn.pipeline import Pipeline

x_train = pd.DataFrame(train_data)
x_test = pd.DataFrame(test_data)

y_train = pd.DataFrame(train_targets)
y_test = pd.DataFrame(test_targets)

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train)

y_train = scaler.fit_transform(y_train)
y_train = pd.DataFrame(y_train)

prob_keras = KerasRegressor(build_fn=build_model_regression, input_data=x_train.shape[1], epochs=70, batch_size=16, verbose=0)

# Define a negative MSE scorer
neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Initialize KFold
kf = KFold(n_splits=4)

# Perform cross-validation
scores = []
for train_index, test_index in kf.split(x_train):
    X_train_fold, X_test_fold = x_train.iloc[train_index], x_train.iloc[test_index]
    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]

    # Fit the model on the training fold
    prob_keras.fit(X_train_fold, y_train_fold)

    # Predict on the test fold
    y_pred = prob_keras.predict(X_test_fold)

    # Calculate and store the negative MSE score
    scores.append(mean_absolute_error(y_test_fold, y_pred))

# Calculate the average negative MSE across all folds
average_neg_mse = np.mean(scores)

print(f"Valor promedio de MAE Error Absoluto medio: {average_neg_mse}")