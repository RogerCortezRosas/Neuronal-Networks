# -*- coding: utf-8 -*-
"""Clasificacion binaria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XASBsUWuucL3YPdp94c69wuPOp0H-Qci
"""

import numpy as np
from keras.datasets import imdb
from keras import models,layers,optimizers

(train_data , train_labels) , (test_data,test_labels) = imdb.load_data(num_words=10000)

train_data.shape# Es en vector de vectores(25000) con numeros que representan las palabras

train_labels# Es un vector que contiene 1 y 0 que respresnta si el review es positivo o negativo

word_index=imdb.get_word_index()

word_index = dict([(values,key) for (key,values) in word_index.items()])

word_index

for _ in train_data[0]:
    print(word_index.get( _ - 3))#toma cada número _ en train_data[0] y le resta 3 (_ - 3).

"""## Función vectorizar

Transformar un vector en matriz
"""

len(train_data)

def vectorizar(sequences, dim=10000):
    """La función vectorizar convierte una lista de secuencias numéricas
      (listas de números) en una matriz binaria (de ceros y unos)."""
    results = np.zeros((len(sequences),dim))
    for i, sequences in enumerate(sequences):
        results[i,sequences]=1
    return results

x_train = vectorizar(train_data)
x_test = vectorizar(test_data)

x_train.ndim

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

"""# Creacion del modelo"""

model = models.Sequential()
model.add(layers.Dense(16, activation='relu' , input_shape=(10000,)))
model.add(layers.Dense(16,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))# es al final solo una neurona ya que clasificara la salida y se una sigmoid ya que es una funcion probabilistica que va de 1 a 0

model.compile(optimizer='rmsprop',# version mejora de descenso del gradiente
              loss='binary_crossentropy',# funcion de perdida
             metrics=['acc'])# metrica a tener encuanta el accuracy

x_val = x_train[:10000]
partial_x_train = x_train[10000:]

y_val = y_train[:10000]
partial_y_train =  y_train[10000:]

"""# Entrenamiento"""

history = model.fit(partial_x_train,
                   partial_y_train,
                   epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))

"""# Analisis de resultados"""

import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']# valores de perdida del set entrenamiento
val_loss_values = history_dict['val_loss']# valores de perdida del set de validacion

fig = plt.figure(figsize=(10,10))
epoch = range(1,len(loss_values)+1)
plt.plot(epoch,loss_values, 'o',label='training')
plt.plot(epoch,val_loss_values, '--',label='val')
plt.legend()
plt.show()

"""Podemos ver en la grafica que en la parte de training el erro se atenua muy rapido mientras que en la de validacion casi no lo hace esto queire decir que hay un overfiting y que el modelo solo se aprendio de memoria los datos

# Regularizacion de overfitting

Modelo mas pequeno
"""

# Resolucion de overfiting
# Modelo menos complejo
model2 = models.Sequential()
model2.add(layers.Dense(4, activation='relu', input_shape=(10000,)))
model2.add(layers.Dense(4, activation='relu'))
model2.add(layers.Dense(1, activation='sigmoid'))

model2.compile(optimizer='rmsprop',# version mejora de descenso del gradiente
              loss='binary_crossentropy',# funcion de perdida
             metrics=['acc'])# metrica a tener encuanta el accuracy

history2 = model2.fit(partial_x_train,
                   partial_y_train,
                   epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))

history2.history

import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

val_loss_values2 = history2.history['val_loss']


fig = plt.figure(figsize=(10,10))
epoch = range(1,len(loss_values)+1)
plt.plot(epoch,val_loss_values2, 'o',label='smaller')
plt.plot(epoch,val_loss_values, '--',label='original')
plt.legend()
plt.show()

"""Vemos que para la funcion de perdida en el set de validacion el modelo mas pequeno tarda mas en hacer overfitting mientras que el mas grande empieza hacer overfiting mucho mas temprano

## Regularizadores L1 y L2
"""

from keras import regularizers

model3 = models.Sequential()
model3.add(layers.Dense(16, activation='relu', input_shape=(10000,),kernel_regularizer=regularizers.l2(0.001)))
model3.add(layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001)))
model3.add(layers.Dense(1, activation='sigmoid'))

model3.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
             metrics=['acc'])

history3 = model3.fit(partial_x_train,
                   partial_y_train,
                   epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))

import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

val_loss_values3 = history3.history['val_loss']


fig = plt.figure(figsize=(10,10))
epoch = range(1,len(loss_values)+1)
plt.plot(epoch,val_loss_values3, 'o',label='regularization')
plt.plot(epoch,val_loss_values, '--',label='original')
plt.legend()
plt.show()

"""## Dropout"""

model4 = models.Sequential()
model4.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model4.add(layers.Dropout(0.5))# para que desactive el 50 % de las neuronas
model4.add(layers.Dense(16, activation='relu'))
model4.add(layers.Dropout(0.5))
model4.add(layers.Dense(1, activation='sigmoid'))

model4.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
             metrics=['acc'])

history4 = model4.fit(partial_x_train,
                   partial_y_train,
                   epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

val_loss_values4 = history4.history['val_loss']


fig = plt.figure(figsize=(10,10))
epoch = range(1,len(loss_values)+1)
plt.plot(epoch,val_loss_values4, 'o',label='dropout')
plt.plot(epoch,val_loss_values, '--',label='original')
plt.legend()
plt.show()

#Evaluamos modelo 4
model4.evaluate(x_test,y_test)

"""## Se combina las 4 regularizaciones"""

model5 = models.Sequential()
model5.add(layers.Dense(5, activation='relu', input_shape=(10000,),kernel_regularizer=regularizers.l2(0.001)))
model5.add(layers.Dropout(0.5))# para que desactive el 50 % de las neuronas
model5.add(layers.Dense(5, activation='relu',kernel_regularizer=regularizers.l2(0.001)))
model5.add(layers.Dropout(0.5))
model5.add(layers.Dense(1, activation='sigmoid'))

model5.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
             metrics=['acc'])

history5 = model5.fit(partial_x_train,
                   partial_y_train,
                   epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

val_loss_values2 = history2.history['val_loss']
val_loss_values3 = history3.history['val_loss']
val_loss_values4 = history4.history['val_loss']
val_loss_values5 = history5.history['val_loss']


fig = plt.figure(figsize=(10,10))
epoch = range(1,len(loss_values)+1)
plt.plot(epoch,val_loss_values5,label='Todas')
plt.plot(epoch,val_loss_values4,label='dropout')
plt.plot(epoch,val_loss_values3,label='Regularizacion')
plt.plot(epoch,val_loss_values2,label='less_Neurons')
plt.plot(epoch,val_loss_values,label='original')
plt.legend()
plt.show()

# Datos de ejemplo
data = {
    "descripcion": ["Modelo con regularización L2", "Modelo con dropout", "Modelo con data augmentation"],
    "accuracy": [0.85, 0.88, 0.90]
}

accuracy_1 = model.evaluate(x_test,y_test)[1]

accuracy_2 = model2.evaluate(x_test,y_test)[1]
accuracy_3 = model3.evaluate(x_test,y_test)[1]
accuracy_4 = model4.evaluate(x_test,y_test)[1]
accuracy_5 = model5.evaluate(x_test,y_test)[1]

# Accuaracy de los modelos
import pandas as pd
dict_accuracy = { 'descripcion':['Modelo original','Modelo con menos neuronas','Modelo con regularizacion L2','Modelo con dropout','Modelo menos neuronas y con regularizacion L2 y Dropout'],
                 'accuracy':[accuracy_1,accuracy_2,accuracy_3,accuracy_4,accuracy_5]}
# Crear DataFrame con índices personalizados
indices = [f"modelo{i}" for i in range(1, len(dict_accuracy["descripcion"]) + 1)]
df = pd.DataFrame(dict_accuracy, index=indices)

df