# -*- coding: utf-8 -*-
"""Mi primer red con Numpy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cwbz6UINv93_sWWLMvKE43mrHHApf2E-
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_gaussian_quantiles

"""# Creacion de la data"""

N = 1000
gaussian_quantiles = make_gaussian_quantiles(mean=None , cov = 0.1 , n_samples = N , n_features = 2 , n_classes = 2 , shuffle = True , random_state = None)

X,Y = gaussian_quantiles

X.shape

Y

Y = Y[:,np.newaxis]

Y

"""# Grafica de la distribucion de la data"""

plt.scatter(X[:,0] , X[:,1] , c = Y[:,0] , s=40 , cmap = plt.cm.Spectral)

"""# Funcion de inicializacion de parametros pesos y bias"""

def initialize_parameters_deep(layer_dims):
    #np.random.seed(1)
    parameters = {}
    L = len(layer_dims)
    for l in range(0, L-1):
        parameters['W' + str(l+1)] = (np.random.rand(layer_dims[l], layer_dims[l+1]) * 2) - 1
        parameters['b' + str(l+1)] = (np.random.rand(1, layer_dims[l+1]) * 2) - 1
    return parameters

layer_dims = [2,4,8,1]
params = initialize_parameters_deep(layer_dims)

params

"""### Funcion de activacion

La función de activación se encarga de ajustar  números de forma que la red pueda aprender cosas más complejas.
"""

# pasar la formula de la funcion sigmoide a python
def sigmoid(x, derivate = False):
    if derivate:
        return np.exp(-x)/(( np.exp(-x) +1)**2)
    else:
        return 1 / (1 + np.exp(-x))

def relu(x,derivate = False):
  if derivate:
    x[ x <= 0 ] = 0
    x[ x > 0 ] = 1
    return x
  else:
    return np.maximum(0,x)

"""Funcion de perdida

Una función de pérdida es como un "termómetro" que mide qué tan mal está funcionando la red.

Usamos el error calculado por la función de pérdida para ajustar los "engranajes internos" de la red, llamados pesos y sesgos. Este ajuste se hace durante el entrenamiento, con un proceso llamado backpropagation.

Ayuda a la red a saber qué hacer para mejorar. Si el error es grande, la red hace cambios más grandes; si es pequeño, hace cambios más pequeños.
"""

#Error cuadratico medio
def mse(y,y_hat,derivate=False):
    if derivate:
        return (y_hat - y)
    else:
        return np.mean((y_hat - y)**2)

"""### Entrenamiento hacia delante forward"""

params['W1'].shape

X.shape

#Producto pnto
np.matmul(X,params['W1']).shape # o X@params['W1']

params['A0'] = X # Asignamos la entrada X a A0

params['Z1'] = np.matmul(params['A0'],params['W1']) + params['b1'] # Producto punto de la entrada con la primera capa
params['A1'] = relu(params['Z1']) # Pasamos por la funcion de activacion

params['Z2'] = np.matmul(params['A1'],params['W2']) + params['b2']# Del resultado anterior se aplica producto punto con la capa 2
params['A2'] = relu(params['Z2'])

params['Z3'] = np.matmul(params['A2'],params['W3']) + params['b3']# Del resultado anterior se aplica producto punto con la capa anterior
params['A3'] = sigmoid(params['Z3']) # porque quiero que me diga la dostribucion estadistica entre 0 y 1

output = params['A3']

output

"""# Funcion de entrenamiento

### Evaluamos las perdidas haciendo las derivadas parciales de las funciones de activacion optimizar con el descenso del gradiente y optimizar los pesos
"""

def train(X_data,lr,params,training=True):
## Forward

    params['A0'] = X_data

    params['Z1'] = np.matmul(params['A0'],params['W1']) + params['b1']
    params['A1'] = relu(params['Z1'])

    params['Z2'] = np.matmul(params['A1'],params['W2']) + params['b2']
    params['A2'] = relu(params['Z2'])

    params['Z3'] = np.matmul(params['A2'],params['W3']) + params['b3']
    params['A3'] = sigmoid(params['Z3'])

    output = params['A3']

    if training:
    # Backpropagation

        params['dZ3'] =  mse(Y,output,True) * sigmoid(params['A3'],True)
        params['dW3'] = np.matmul(params['A2'].T,params['dZ3'])

        params['dZ2'] = np.matmul(params['dZ3'],params['W3'].T) * relu(params['A2'],True)
        params['dW2'] = np.matmul(params['A1'].T,params['dZ2'])

        params['dZ1'] = np.matmul(params['dZ2'],params['W2'].T) * relu(params['A1'],True)
        params['dW1'] = np.matmul(params['A0'].T,params['dZ1'])


        ## Gradinet Descent:

        params['W3'] = params['W3'] - params['dW3'] * lr
        params['b3'] = params['b3'] - (np.mean(params['dZ3'],axis=0, keepdims=True)) * lr

        params['W2'] = params['W2'] - params['dW2'] * lr
        params['b2'] = params['b2'] - (np.mean(params['dZ2'],axis=0, keepdims=True)) * lr

        params['W1'] = params['W1'] -params['dW1'] * lr
        params['b1'] = params['b1'] - (np.mean(params['dZ1'],axis=0, keepdims=True)) * lr

    return output

"""Capa de entrenamiento"""

layer_dims = [2,4,8,1]# las cpas de la neurona Arquitectura de la red
# 1er capa = 2 neurona , 2da capa = 4 neuronas , 3er capa = 8 neuronas ,4ta capa = salida
params = initialize_parameters_deep(layer_dims) # data
errors = []
"""Piensa en el learning rate como la velocidad a la que la red aprende.
Cada vez que la red calcula un error (con la función de pérdida), el learning rate decide qué tan grandes
serán los pasos que toma para corregir ese error."""
"""El entrenamiento por épocas significa que la red neuronal pasa varias veces por todos los datos de entrenamiento para aprender mejor.
 Su propósito es dar tiempo al modelo para ajustar sus parámetros y minimizar el error"""
for _ in range(30000): #30000 epocas
  output = train(X,0.001,params) #0.001 learning rate es un número pequeño que controla qué tan grandes son los ajustes que realiza una red neuronal en sus parámetros (pesos y sesgos) durante el proceso de entrenamiento.
  if _ %50 == 0:
    print(mse(Y,output))
    errors.append(mse(Y,output))

"""## Grafica de la degradacion el error"""

plt.plot(errors)

"""### Ahora predecimos con data nueva"""

data_test = (np.random.rand(1000,2) *2 ) - 1
y = train(data_test,0.0001,params,training=False)#False para solo predecir

y = np.where(y>= 0.5 ,1,0)

plt.scatter(data_test[:,0] , data_test[:,1],c=y[:,0] , s=40 , cmap=plt.cm.Spectral)

"""Conclusion la red aprendia que en el centro va una clase y en los extremos va otra"""