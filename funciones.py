# -*- coding: utf-8 -*-
"""Funciones.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11AchCz5_wnYOuDuAsrCJy8_stXPx4xOI
"""

import numpy as np
import matplotlib.pyplot as plt

# pasar la formula de la funcion sigmoide a python
def sigmoide (x):
 return 1 / (1 + np.exp(-x))

a = np.linspace(10,-10,100)

plt.plot(a,sigmoide(a))

"""Ahora vemos la funcion Step la cual si los valores mayores o igual
a cero devuelve 1 y los valore menores de cero son 0

"""

def step(a):

  return np.piecewise(a,[a<0.0 , a>0.0],[0,1])

plt.plot(a,step(a))

"""# Funcion Relu"""

def relu(x):
    return np.maximum(0, x)

b = np.linspace(-10, 10, 100)

plt.plot(b,relu(b))

"""# Funcion Tangente  tanh(x)"""

def tanh(x):
    return np.tanh(x)

plt.plot(b,tanh(b))

"""# Funcion de perdida
 Toma los valores reales y los valores predichos para que calcule que tan desviado estor del dato real ( Me va a decir que tan buena fue la prediccion).
  
 Si el valor es muy alto significa que la prediccion no es buena y por el contrario si el valor es muy bajo quiere decir que la prediccion es buena
"""

#Error cuadratico medio
def mse(y_pred, y_real,derivate=False):

  if derivate:
    return y_pred - y_real
  else:
    error = np.mean((y_pred - y_real)**2)
    return error

real = np.array([0,0,1,1])
predic = np.array([0.9,0.8,1,0.3])

mse(predic,real)

"""# Cuando una funcion matematica se deriva se optimiza , las redes actualizan los pesos en base a la derivada de la funcion  de perdida y de activacion

## Cuando una funcion de perdidad se deriva se encuntra su pendiente  , cuando la pendiente es cero se dice que es el punto en donde no hay error (tambien puede ser una cresta)  por lo tanto mientras una funcion de perdida se encutnre mas a cero significa que el modelo es mejor

## El descenso del gradiente es como bajar una montaña en la oscuridad. Usamos la "inclinación" (derivada) para decidir en qué dirección movernos, y damos pasos pequeños para asegurarnos de no saltarnos el valle. Este método se utiliza en el aprendizaje automático para ajustar los parámetros de un modelo y minimizar el error
"""

